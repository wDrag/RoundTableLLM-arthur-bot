## Models: Master vs Specialists
### Master
- The master service does not run a model directly. It is an HTTP orchestrator that routes requests to specialists (agents) and aggregates their outputs.

### Specialists (Agents)
- Analyst, Builder, Critic are the specialists.
- Each specialist calls callLLM with its mode and messages.
- Provider selection and per-agent model overrides are read from llm.config.json. If unset, auto-picks based on available keys.

### Default Models (if no per-agent model set)
- OpenAI: gpt-5.2-codex
- Anthropic: claude-3-5-sonnet-latest
- Grok: grok-2-latest

### Where Models Are Chosen
- Provider auto-pick in src/llm/ensemble.ts
- Per-agent provider/model overrides in llm.config.json (loaded by src/config.ts)
- Provider defaults in src/llm/providers/openai.ts, src/llm/providers/anthropic.ts, src/llm/providers/grok.ts

## Requirements Comparison (Requested vs Repo)
### Core Stack
- Node 20+, TypeScript ESM NodeNext, pnpm, Fastify + Zod: ✅

### API Contract
- POST /api/chat request/response shape preserved: ✅
- GET /health -> { ok: true }: ✅
- Auth behavior with MASTER_API_KEY: ✅
- Always returns JSON { reply }: ✅

### Orchestration
- Router -> Runner -> Aggregator -> Reply: ✅
- Always includes critic: ✅
- Runner defaults (concurrency=2, timeout=4500ms): ✅
- Aggregator invalidation logic (risk>=0.60 && confidence<=0.45): ✅

### LLM Adapter Layer
- Providers: OpenAI, Anthropic, Grok, Dummy: ✅
- AbortController timeout in providers: ✅
- Auto-pick provider by keys; fallback to dummy on failure: ✅
- No secret logging: ✅
- Deterministic fallback always available: ✅

### Env Vars
- ANTHROPIC_API_KEY: ✅
- XAI_API_KEY: ✅
- Provider/model overrides moved to llm.config.json: ✅

### NodeNext ESM Import Rules
- Internal imports use .js extensions: ✅

### Docker
- Multi-stage build and healthcheck: ✅
- Runs dist/server.js directly (no PM2 dependency): ✅

## Environment Example
- Added .env.example with all supported variables.

## Environment Variables (Detailed)
### Server
- PORT: Port the HTTP server listens on. Default 8000.
- LOG_LEVEL: Fastify logger level. Examples: trace, debug, info, warn, error. Default info.

### Auth
- MASTER_API_KEY: If set, POST /api/chat requires Authorization: Bearer <MASTER_API_KEY>. If unset, requests are unauthenticated.

### Provider Keys
- OPENAI_API_KEY: Enables OpenAI provider.
- ANTHROPIC_API_KEY: Enables Anthropic provider.
- XAI_API_KEY: Enables Grok provider.

### Model/Weight Config File
- llm.config.json controls per-agent provider, per-agent model, and weight tables.

## Weighting Configuration
- Weight tables and invalidation thresholds are defined in llm.config.json.
- Task types included: PROMPT_ENGINEERING, VERBAL_REASONING, TECHNICAL_EXECUTION, VISUAL_ANALYSIS.

## Weight Wiring Proof
- Weights are loaded from llm.config.json by src/config.ts and accessible via getWeights().
- Weight keys are now real model names (e.g., gpt-5.2-codex, claude-3-5-sonnet-implementer, grok-2-latest).

## Latest Checks
- Docker build: roundtable-master:test
- Docker run: /health -> { ok: true }, /api/chat -> { reply: "[builder] ping" }
